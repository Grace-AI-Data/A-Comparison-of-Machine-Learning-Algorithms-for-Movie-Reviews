{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3725d649-073a-489c-9e1a-ca6b4fbd51c4",
   "metadata": {},
   "source": [
    "### Using Natural Language Processing to Preprocess and Clean Movie reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe29874-e320-4b35-8ff1-1fbc7e8a3323",
   "metadata": {},
   "source": [
    "Use a machine learning classifiers to determine the sentiment of processed movie reviews data\n",
    "Building NLP Sentiment Analyzer, Loading and Preprocessing Data, Training Your Classifier, Classifying Reviews, Connecting the Pipeline.\n",
    "\n",
    "Data: Large Movie Review Dataset(https://ai.stanford.edu/~amaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e44ba4-2fa2-4ad6-b0db-f939ff0771fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n",
      "Runing on PyMC3 v3.11.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import spacy\n",
    "import arviz as az\n",
    "from spacy.util import minibatch, compounding\n",
    "import pandas as pd\n",
    "nlp = spacy.load('en',parse=True,tag=True, entity=True)\n",
    "spacy.load('en')\n",
    "\n",
    "import scipy\n",
    "print(scipy.__version__)\n",
    "import pymc3 as pm\n",
    "print(f\"Runing on PyMC3 v{pm.__version__}\")\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2249f9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26207d26-e598-426e-991a-9f28fd33a87b",
   "metadata": {},
   "source": [
    "### Model 1: Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "72e4b7b4-c1a4-490e-9233-ae14cc5362d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_REVIEW = \"\"\"\n",
    "Transcendently beautiful in moments outside the office, it seems almost\n",
    "sitcom-like in those scenes. When Toni Colette walks out and ponders\n",
    "life silently, it's gorgeous.<br /><br />The movie doesn't seem to decide\n",
    "whether it's slapstick, farce, magical realism, or drama, but the best of it\n",
    "doesn't matter. (The worst is sort of tedious - like Office Space with less\n",
    "humor.)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "eval_list = []\n",
    "\n",
    "\n",
    "def train_model_CNN(\n",
    "    training_data: list, test_data: list, iterations: int = 20\n",
    ") -> None:\n",
    "    # Build pipeline\n",
    "    nlp = spacy.load(r'/Users/grace/opt/anaconda3/lib/python3.9/site-packages/en_core_web_sm/en_core_web_sm-2.3.1') #load build-in pipeline\n",
    "    if \"textcat\" not in nlp.pipe_names: # check textcat componet is already available\n",
    "        textcat = nlp.create_pipe(  #textcategorizer\n",
    "            \"textcat\", config={\"architecture\": \"simple_cnn\"}\n",
    "        )\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")# get_pipe() to assign it to a variable\n",
    "\n",
    "    textcat.add_label(\"pos\")# add the labels from data\n",
    "    textcat.add_label(\"neg\")\n",
    "\n",
    "    \n",
    "    # Train only textcat\n",
    "    training_excluded_pipes = [\n",
    "        pipe for pipe in nlp.pipe_names if pipe != \"textcat\"\n",
    "    ]\n",
    "    with nlp.disable_pipes(training_excluded_pipes): #content manager\n",
    "        #As we are only focusing on entity extraction, we will disable all other pipeline components to train our model for ner only using nlp.disable_pipes()\n",
    "        optimizer = nlp.begin_training() #returns the initial optimizer function\n",
    "        # Training loop\n",
    "        print(\"Beginning training\")\n",
    "        print(\"Loss\\tPrecision\\tRecall\\tF-scoret\\tAccuracy\")\n",
    "        batch_sizes = compounding(  #create a generator\n",
    "            4.0, 32.0, 1.001\n",
    "        )  # A generator that yields infinite series of input numbers(batch_sizes)\n",
    "        for i in range(iterations):\n",
    "            print(f\"Training iteration {i}\")\n",
    "            loss = {}\n",
    "            random.shuffle(training_data)\n",
    "            \n",
    "            batches = minibatch(training_data, size=batch_sizes)\n",
    "            for batch in batches:\n",
    "                text, labels = zip(*batch)\n",
    "                nlp.update(text, labels, drop=0.2, sgd=optimizer, losses=loss) #update the weights of the underlying model\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                evaluation_results = evaluate_model(\n",
    "                    tokenizer=nlp.tokenizer,\n",
    "                    textcat=textcat,\n",
    "                    test_data=test_data,\n",
    "                )\n",
    "                print(\n",
    "                    f\"{loss['textcat']}\\t{evaluation_results['precision']}\"\n",
    "                    f\"\\t{evaluation_results['recall']}\"\n",
    "                    f\"\\t{evaluation_results['f-score']}\"\n",
    "                    f\"\\t{evaluation_results['accuracy']}\"\n",
    "                )\n",
    "\n",
    "    # Save model\n",
    "    with nlp.use_params(optimizer.averages):\n",
    "        nlp.to_disk(\"model_artifacts\")\n",
    "\n",
    "\n",
    "def evaluate_model(tokenizer, textcat, test_data: list) -> dict:\n",
    "    reviews, labels = zip(*test_data)\n",
    "    reviews = (tokenizer(review) for review in reviews)\n",
    "    true_positives = 0\n",
    "    false_positives = 1e-8  # Can't be 0 because of presence in denominator\n",
    "    true_negatives = 0\n",
    "    false_negatives = 1e-8\n",
    "    for i, review in enumerate(textcat.pipe(reviews)):\n",
    "        true_label = labels[i][\"cats\"]\n",
    "        for predicted_label, score in review.cats.items():\n",
    "            # Every cats dictionary includes both labels, you can get all\n",
    "            # the info you need with just the pos label\n",
    "            if predicted_label == \"neg\":\n",
    "                continue\n",
    "            if score >= 0.5 and true_label[\"pos\"]:\n",
    "                true_positives += 1\n",
    "            elif score >= 0.5 and true_label[\"neg\"]:\n",
    "                false_positives += 1\n",
    "            elif score < 0.5 and true_label[\"neg\"]:\n",
    "                true_negatives += 1\n",
    "            elif score < 0.5 and true_label[\"pos\"]:\n",
    "                false_negatives += 1\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    accuracy = (true_positives+true_negatives)/(true_positives + false_negatives+false_positives + true_negatives)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f_score = 0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f-score\": f_score, \"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "def test_model(loaded_model, input_data: str = TEST_REVIEW):\n",
    "    #  Load saved trained model\n",
    "    #loaded_model = spacy.load(\"model_artifacts\")\n",
    "    # Generate prediction\n",
    "    parsed_text = loaded_model(input_data)\n",
    "\n",
    "    output_labels = []\n",
    "    # Determine prediction to return\n",
    "    if parsed_text.cats[\"pos\"] > parsed_text.cats[\"neg\"]:\n",
    "        #prediction = \"Positive\"\n",
    "        #score = parsed_text.cats[\"pos\"]\n",
    "        output_labels.append(1)\n",
    "    else:\n",
    "        #prediction = \"Negative\"\n",
    "        #score = parsed_text.cats[\"neg\"]\n",
    "        output_labels.append(0)\n",
    "    #print(\n",
    "        #f\"Review text: {input_data}\\nPredicted sentiment: {prediction}\"\n",
    "        #f\"\\tScore: {score}\"\n",
    "    #)\n",
    "\n",
    "    return output_labels\n",
    "\n",
    "\n",
    "    #constrcut the directory structure of data, look for and open text files, then appende a tuple of \n",
    "    #the contents and a label dictionary to the reviews list.\n",
    "def load_training_data(\n",
    "    data_directory: str = \"/Users/grace/Desktop/aclImdb/train\", split: float = 0.8, limit: int = 0\n",
    ") -> tuple:\n",
    "    # Load from files\n",
    "    reviews = []\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        labeled_directory = f\"{data_directory}/{label}\"\n",
    "        for review in os.listdir(labeled_directory):\n",
    "            if review.endswith(\".txt\"):\n",
    "                with open(f\"{labeled_directory}/{review}\") as f:\n",
    "                    text = f.read()\n",
    "                    text = text.replace(\"<br />\", \"\\n\\n\") #replace html tags with newlines\n",
    "                    if text.strip(): #remove all leading and tailing whitespace\n",
    "                        spacy_label = {\n",
    "                            \"cats\": {\n",
    "                                \"pos\": \"pos\" == label,\n",
    "                                \"neg\": \"neg\" == label,\n",
    "                            }\n",
    "                        }\n",
    "                        reviews.append((text, spacy_label))\n",
    "   \n",
    "    # shuffle data to eliminate any possible bias from the order in which training data is loaded.\n",
    "    random.shuffle(reviews) \n",
    "\n",
    "    if limit:\n",
    "        reviews = reviews[:limit]\n",
    "    split = int(len(reviews) * split)\n",
    "    return reviews[:split], reviews[split:] #convert the split to a number of items that define the split boundary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ffbc789-0093-4fe3-927c-4f6c46aa430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_training_data(dataset):\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    \n",
    "    for text, spacy_label in dataset:\n",
    "        training_data.append(text)\n",
    "        categories = spacy_label['cats']\n",
    "        if categories['pos'] == True:\n",
    "            training_labels.append(1)\n",
    "        else:\n",
    "            training_labels.append(0)\n",
    "    \n",
    "    return training_data, training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3fa33aeb-1e43-4e9a-a78e-3a8a336bb213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:  12000\n",
      "Test dataset:  4000\n",
      "Validation dataset:  4000\n"
     ]
    }
   ],
   "source": [
    "train, test = load_training_data(limit=20000)\n",
    "validation = train[12000:]\n",
    "train = train[:12000]\n",
    "training_texts, training_labels = label_training_data(train)\n",
    "test_texts, test_labels = label_training_data(test)\n",
    "print('Training dataset: ', len(training_texts))\n",
    "print('Test dataset: ', len(test_texts))\n",
    "print('Validation dataset: ', len(validation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16737e89-5138-4dde-bb7d-052f1eedf8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Allow yourself to be transported to a different, old school kind of storytelling. Scoop is classic Woody Allen.\\n\\n\\n\\nAllen's latest muse, Scarlett Johansson (who also appeared in last year's Match Point, also by Allen), is surprisingly able to tone down her sultry sex kitten appeal and transform into a normal looking student-type with the aid of nerdish glasses and outfits but still fails to make the audience believe how Hugh Jackman's lordly character can be so smitten by her, given the royal's background (don't worry, no spoilers here). There are no grand transformations for Johansson's character here, as she consistently plays the same character throughout despite the script saying otherwise. You even forgive her character's apparent lack of logic, continuing an affair with a suspected serial killer, simply because he is His Royal Hotness Jackman, who is refreshing to see sans the Wolverine duds.\\n\\n\\n\\nIf anything, consistency is what the 70-year old Allen is all about. He continues to tell his stories on celluloid in the same way he always has; as if he's never been exposed to modern film-making, which is probably what makes his quiet, simple films appealing. They never seem to aim for a specific market; as if Allen makes movies to his taste alone, whether the public likes it or not.\", \"absolutely trash. i liked Halloween and from then on johnny's been in a downward spiral. this is about the pits. we get it john. pro-lifers are scary! you don't have to make a shitty film that bores the hell out of me to 'tell' me.\\n\\n\\n\\nThe pacing is way off here. It feels like john didn't have much to work with here. to his credit it looks like he did not write this junk. There are countless times where the camera just sits and waits for the actors to look dumb or say something dumb. i love the long cut. too bad carpenter doesn't know how to employ it. he needs to bunk up with Herzog and Fassbinder 30 years ago. Please John, stop making a fool of yourself and boring me to death!\"]\n",
      "[1, 0, 1, 1, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(training_texts[:2])\n",
    "print(training_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1a07cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "\n",
    "def compute_test_set(loaded_model, test_texts, test_labels):\n",
    "    predicted_labels = []\n",
    "\n",
    "    for text in test_texts:\n",
    "        predicted_labels += test_model(loaded_model, text)\n",
    "\n",
    "\n",
    "    print(metrics.classification_report(test_labels, predicted_labels))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "91d6c090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Beginning training\n",
      "Loss\tPrecision\tRecall\tF-scoret\tAccuracy\n",
      "Training iteration 0\n",
      "15.651854295749217\t0.8407851690248594\t0.770229770225923\t0.8039624608925758\t0.8119999999959401\n",
      "Training iteration 1\n",
      "0.7328757226350717\t0.8502109704596509\t0.8051948051907832\t0.8270908157987322\t0.8314999999958426\n",
      "Training iteration 2\n",
      "0.1599758323172864\t0.8516397709481955\t0.8171828171787354\t0.8340555697128013\t0.8372499999958138\n",
      "Training iteration 3\n",
      "0.057379591518838424\t0.8528025144009806\t0.8131868131827513\t0.8325236512358347\t0.8362499999958188\n",
      "Training iteration 4\n",
      "0.022719194933415565\t0.8559411146116933\t0.8131868131827513\t0.8340163934383503\t0.8379999999958101\n",
      "Training iteration 5\n",
      "0.01671620870888546\t0.860576923072326\t0.8046953046912853\t0.8316985028351487\t0.8369999999958151\n",
      "Training iteration 6\n",
      "0.013931921963830973\t0.8644158628035133\t0.8056943056902813\t0.8340227507712823\t0.8394999999958025\n",
      "Training iteration 7\n",
      "0.01676963727572911\t0.8649225840850778\t0.8091908091867673\t0.8361290322537491\t0.8412499999957938\n",
      "Training iteration 8\n",
      "0.013305279869655351\t0.8614330874559462\t0.8166833166792374\t0.8384615384572387\t0.8424999999957875\n",
      "Training iteration 9\n",
      "0.00928386278576454\t0.8635881103988132\t0.8126873126832533\t0.8373648996354227\t0.84199999999579\n",
      "Training iteration 10\n",
      "0.007481637860706769\t0.8645778013761839\t0.8131868131827513\t0.8380952380909237\t0.8427499999957863\n",
      "Training iteration 11\n",
      "0.007576275440271729\t0.8626723223708236\t0.8126873126832533\t0.8369341563742956\t0.8414999999957925\n",
      "Training iteration 12\n",
      "0.005934120505493468\t0.8613756613711039\t0.8131868131827513\t0.8365878725547965\t0.840999999995795\n",
      "Training iteration 13\n",
      "0.006359969545883359\t0.8547717842279318\t0.8231768231727115\t0.8386768447794469\t0.8414999999957925\n",
      "Training iteration 14\n",
      "0.007034307754580027\t0.8503366131494028\t0.8201798201757234\t0.8349860157598018\t0.8377499999958113\n",
      "Training iteration 15\n",
      "0.006534868762003043\t0.8529106029061699\t0.8196803196762255\t0.8359653591399085\t0.838999999995805\n",
      "Training iteration 16\n",
      "0.00631672408582773\t0.8518711018666744\t0.8186813186772294\t0.8349465104389459\t0.8379999999958101\n",
      "Training iteration 17\n",
      "0.005918927021669518\t0.8499999999956186\t0.8236763236722094\t0.8366311516954003\t0.838999999995805\n",
      "Training iteration 18\n",
      "0.00531653681438371\t0.8516228748024132\t0.8256743256702015\t0.8384478823188514\t0.8407499999957962\n",
      "Training iteration 19\n",
      "0.0031855228661576973\t0.8532637075673459\t0.8161838161797393\t0.8343119734448083\t0.8377499999958113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandas.plotting._core.PlotAccessor object at 0x7fe7c9686f40>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#train, test = load_training_data(limit=20000)\n",
    "print(\"Training model\") \n",
    "train_model_CNN(train[:6000], validation[:6000])\n",
    "df = pd.DataFrame(eval_list)\n",
    "pd.DataFrame.plot(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3429e6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84      1998\n",
      "           1       0.85      0.83      0.84      2002\n",
      "\n",
      "    accuracy                           0.84      4000\n",
      "   macro avg       0.84      0.84      0.84      4000\n",
      "weighted avg       0.84      0.84      0.84      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model = spacy.load(\"model_artifacts\")\n",
    "print(\"Testing model\")\n",
    "compute_test_set(loaded_model, test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7324c727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pandas.plotting._core.PlotAccessor object at 0x7fe80587ca30>\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(eval_list)\n",
    "print(pd.DataFrame.plot(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779f8bd-ed36-4278-8d1f-d9080e64e636",
   "metadata": {},
   "source": [
    "### Model 2: Naive Bayese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "41eb147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c1406c91-0401-4d4e-b07a-4183407949f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "NB = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b531bb25-fca6-45c0-937c-30a5c282bf69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model \n",
    "NB.fit(training_texts, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a8ba3d58-e5a9-4e89-8617-641107327f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_NB = NB.predict(test_texts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1b4e125b-3aa0-4dff-bd52-33544e8c2321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.86      1998\n",
      "           1       0.88      0.81      0.84      2002\n",
      "\n",
      "    accuracy                           0.85      4000\n",
      "   macro avg       0.85      0.85      0.85      4000\n",
      "weighted avg       0.85      0.85      0.85      4000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1776,  222],\n",
       "       [ 380, 1622]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit-learn provides further utilities for more detailed performance analysis\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(test_labels, predicted_NB))\n",
    "\n",
    "metrics.confusion_matrix(test_labels, predicted_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7181bfbe-cc98-42fe-adaa-ca563d958c04",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model 3: Support Vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6ed53ced-cb23-4caa-a8a2-b1527a5fefd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.853"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#linear support vector machine (SVM),  a bit slower than naïve Bayes\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "SV = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None)),\n",
    " ])\n",
    "\n",
    "SV.fit(training_texts, training_labels)\n",
    "predicted_SV = SV.predict(test_texts)\n",
    "np.mean(predicted_SV == test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cd1b8857-0115-4bf8-af03-a0e055ffbf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_SV = SV.predict(test_texts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4a6f9bac-e65d-436c-b080-b897fdeb0a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.81      0.85      1998\n",
      "           1       0.83      0.89      0.86      2002\n",
      "\n",
      "    accuracy                           0.85      4000\n",
      "   macro avg       0.86      0.85      0.85      4000\n",
      "weighted avg       0.86      0.85      0.85      4000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1622,  376],\n",
       "       [ 212, 1790]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit-learn provides further utilities for more detailed performance analysis\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(test_labels, predicted_SV))\n",
    "\n",
    "metrics.confusion_matrix(test_labels, predicted_SV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae2eb8-f4ea-431a-904a-a71415043def",
   "metadata": {},
   "source": [
    "### Model 4: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "36c30952-cb5e-4b33-b4cc-6b21bc37c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b6bae8b0-2246-4ac6-9b5d-35b03d93bacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 RandomForestClassifier(criterion='entropy', max_depth=50,\n",
       "                                        min_samples_leaf=4, min_samples_split=3,\n",
       "                                        n_estimators=20))])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier(n_estimators=20, \n",
    "                                   criterion = 'entropy',\n",
    "                                   max_depth=50, \n",
    "                                   min_samples_leaf=4,\n",
    "                                   min_samples_split=3))])\n",
    "RF.fit(training_texts, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5661a1d5-42c2-4bbf-b119-d5b5ccd09799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80      1998\n",
      "           1       0.80      0.81      0.80      2002\n",
      "\n",
      "    accuracy                           0.80      4000\n",
      "   macro avg       0.80      0.80      0.80      4000\n",
      "weighted avg       0.80      0.80      0.80      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds_RF = RF.predict(test_texts)\n",
    "print(metrics.classification_report(test_labels, preds_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addbb985-3b2a-4f8f-9e53-534fba94083b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
